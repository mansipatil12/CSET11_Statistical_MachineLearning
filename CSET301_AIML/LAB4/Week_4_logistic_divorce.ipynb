{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMHjtVPbyaKP"
      },
      "source": [
        "## Logistic Regression Model for Divorce Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kktr-4GPI5ou"
      },
      "source": [
        "## Part 1.1: Implement  logistic regression from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJi26z8awmSD"
      },
      "source": [
        "### Logistic regression\n",
        "Logistic regression uses an equation as the representation, very much like linear regression.\n",
        "\n",
        "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
        "\n",
        "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
        "\n",
        "#### Dataset\n",
        "The dataset is available at <strong>\"data/divorce.csv\"</strong> in the respective challenge's repo.<br>\n",
        "<strong>Original Source:</strong> https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set. Dataset is based on rating for questionnaire filled by people who already got divorse and those who is happily married.<br><br>\n",
        "\n",
        "[//]: # \"The dataset is available at http://archive.ics.uci.edu/ml/machine-learning-databases/00520/data.zip. Unzip the file and use either CSV or xlsx file.<br>\"\n",
        "\n",
        "\n",
        "#### Features (X)\n",
        "1. Atr1 - If one of us apologizes when our discussion deteriorates, the discussion ends. (Numeric | Range: 0-4)\n",
        "2. Atr2 - I know we can ignore our differences, even if things get hard sometimes. (Numeric | Range: 0-4)\n",
        "3. Atr3 - When we need it, we can take our discussions with my spouse from the beginning and correct it. (Numeric | Range: 0-4)\n",
        "4. Atr4 - When I discuss with my spouse, to contact him will eventually work. (Numeric | Range: 0-4)\n",
        "5. Atr5 - The time I spent with my wife is special for us. (Numeric | Range: 0-4)\n",
        "6. Atr6 - We don't have time at home as partners. (Numeric | Range: 0-4)\n",
        "7. Atr7 - We are like two strangers who share the same environment at home rather than family. (Numeric | Range: 0-4)\n",
        "\n",
        "&emsp;.<br>\n",
        "&emsp;.<br>\n",
        "&emsp;.<br>\n",
        "<br>\n",
        "54. Atr54 - I'm not afraid to tell my spouse about her/his incompetence. (Numeric | Range: 0-4)\n",
        "<br><br>\n",
        "Take a look above at the source of the original dataset for more details.\n",
        "\n",
        "#### Target (y)\n",
        "55. Class: (Binary | 1 => Divorced, 0 => Not divorced yet)\n",
        "\n",
        "#### Objective\n",
        "To gain understanding of logistic regression through implementing the model from scratch\n",
        "\n",
        "#### Tasks\n",
        "- Download and load the data (csv file contains ';' as delimiter)\n",
        "- Add column at position 0 with all values=1 (pandas.DataFrame.insert function). This is for input to the bias $w_0$\n",
        "- Define X matrix (independent features) and y vector (target feature) as numpy arrays\n",
        "- Print the shape and datatype of both X and y\n",
        "[//]: # \"- Dataset contains missing values, hence fill the missing values (NA) by performing missing value prediction\"\n",
        "[//]: # \"- Since the all the features are in higher range, columns can be normalized into smaller scale (like 0 to 1) using different methods such as scaling, standardizing or any other suitable preprocessing technique (sklearn.preprocessing.StandardScaler)\"\n",
        "- Split the dataset into 85% for training and rest 15% for testing (sklearn.model_selection.train_test_split function)\n",
        "- Follow logistic regression class and fill code where highlighted:\n",
        "    - Write sigmoid function to predict probabilities\n",
        "    - Write cross entropy or log loss function (i.e. negative log likelihood)\n",
        "    - Write fit function where gradient descent is implemented\n",
        "    - Write predict_proba function where we predict probabilities for input data\n",
        "- Train the model\n",
        "- Write function for calculating accuracy\n",
        "- Compute accuracy on train and test data\n",
        "\n",
        "#### Further Fun (will not be evaluated)\n",
        "- Play with learning rate and max_iterations\n",
        "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
        "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
        "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
        "- Print other classification metrics such as:\n",
        "    - classification report (sklearn.metrics.classification_report),\n",
        "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
        "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
        "\n",
        "#### Helpful links\n",
        "- How Logistic Regression works: https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
        "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "- Training testing splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "21J6cpd_wmSE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9av7W-wowmSI"
      },
      "outputs": [],
      "source": [
        "# Read the data from local cloud directory\n",
        "data = pd.read_csv('divorce_data.csv')\n",
        "# Set delimiter to semicolon(;) in case of unexpected results\n",
        "data = pd.read_csv('divorce_data.csv', delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kmXolHwEI5o8"
      },
      "outputs": [],
      "source": [
        "# Add column which has all 1s\n",
        "data['intercept'] = 1\n",
        "\n",
        "# The idea is that weight corresponding to this column is equal to intercept\n",
        "# This way it is efficient and easier to handle the bias/intercept term\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eV1jGAQxwmSP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  Q47  Q48  Q49  Q50  Q51  Q52  \\\n",
            "0   2   2   4   1   0   0   0   0   0    0  ...    1    3    3    3    2    3   \n",
            "1   4   4   4   4   4   0   0   4   4    4  ...    2    3    4    4    4    4   \n",
            "2   2   2   2   2   1   3   2   1   1    2  ...    2    3    1    1    1    2   \n",
            "3   3   2   3   2   3   3   3   3   3    3  ...    2    3    3    3    3    2   \n",
            "4   2   2   1   1   1   1   0   0   0    0  ...    1    2    3    2    2    2   \n",
            "\n",
            "   Q53  Q54  Divorce  intercept  \n",
            "0    2    1        1          1  \n",
            "1    2    2        1          1  \n",
            "2    2    2        1          1  \n",
            "3    2    2        1          1  \n",
            "4    1    0        1          1  \n",
            "\n",
            "[5 rows x 56 columns]\n"
          ]
        }
      ],
      "source": [
        "# Print the dataframe rows just to see some samples\n",
        "print(data.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "joRU6dWxwmSR"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index(['DIVORCE', 'INTERCEPT'], dtype='object')] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define X (input features) and y (output feature) \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m XY \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDIVORCE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mINTERCEPT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['DIVORCE', 'INTERCEPT'], dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "# Define X (input features) and y (output feature) \n",
        "XY = data[['DIVORCE', 'INTERCEPT']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAyM-CYCwmSU"
      },
      "outputs": [],
      "source": [
        "X_shape = \n",
        "X_type  = \n",
        "y_shape = \n",
        "y_type  = \n",
        "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
        "print(f'y: Type-{y_type}, Shape-{y_shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJrEelFzI5pA"
      },
      "source": [
        "<strong>Expected output: </strong><br><br>\n",
        "\n",
        "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)<br>\n",
        "y: Type-<class 'numpy.ndarray'>, Shape-(170,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdLIVOm127-z"
      },
      "outputs": [],
      "source": [
        "# Check and fill any missing values if any\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En9Kb9dh2-wm"
      },
      "outputs": [],
      "source": [
        "# Perform standarization (if required)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8WF-EqO3BEa"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing here\n",
        "X_train, X_test, y_train, y_test = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acCATJhI3FdH"
      },
      "outputs": [],
      "source": [
        "# Print the shape of features and target of training and testing: X_train, X_test, y_train, y_test\n",
        "X_train_shape = \n",
        "y_train_shape = \n",
        "X_test_shape  = \n",
        "y_test_shape  = \n",
        "\n",
        "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
        "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
        "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSa7cW-NwmSd"
      },
      "source": [
        "##### Let us start implementing logistic regression from scratch. Just follow code cells, see hints if required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxCdzxBI5pF"
      },
      "source": [
        "##### We will build a LogisticRegression class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQgpspPcI5pH"
      },
      "outputs": [],
      "source": [
        "# DO NOT EDIT ANY VARIABLE OR FUNCTION NAME(S) IN THIS CELL\n",
        "# Let's try more object oriented approach this time :)\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
        "        '''Initialize variables\n",
        "        Args:\n",
        "            learning_rate  : Learning Rate\n",
        "            max_iterations : Max iterations for training weights\n",
        "        '''\n",
        "        # Initialising all the parameters\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.cross_entropy_error    = [] # Summary of the cross entroy or log loss (i.e. negative log-likelihood)\n",
        "        \n",
        "        # Define epsilon because log(0) is not defined\n",
        "        self.eps = 1e-7\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''Sigmoid function: f:R->(0,1)\n",
        "        Args:\n",
        "            z : A numpy array (num_samples,)\n",
        "        Returns:\n",
        "            A numpy array where sigmoid function applied to every element\n",
        "        '''\n",
        "        ### START CODE HERE\n",
        "        sig_z = \n",
        "        ### END CODE HERE\n",
        "        \n",
        "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
        "        return sig_z\n",
        "    \n",
        "    def cross_entropy(self, y_true, y_pred):\n",
        "        '''Calculates cross_entropy or log loss (negative log-likelihood) estimate\n",
        "        Remember: -[y * log(yh) + (1-y) * log(1-yh)]\n",
        "        Note: Cross_entropy or log-loss is defined for multiple classes as well, but for this dataset\n",
        "        \n",
        "        Args:\n",
        "            y_true : Numpy array of actual truth values (num_samples,)\n",
        "            y_pred : Numpy array of predicted values (num_samples,)\n",
        "        Returns:\n",
        "            cross_entropy(or log loss which is the negative Log-likelihood) scalar value\n",
        "        '''\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        cross_entropy = \n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return cross_entropy\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        '''Trains logistic regression model using gradient descent\n",
        "        to gain minimum cross_entropy/log-loss on the training data\n",
        "        Args:\n",
        "            X : Numpy array (num_examples, num_features)\n",
        "            y : Numpy array (num_examples, )\n",
        "        Returns: VOID\n",
        "        '''\n",
        "        \n",
        "        num_examples = X.shape[0]\n",
        "        num_features = X.shape[1]\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        \n",
        "        # Initialize weights with appropriate shape\n",
        "        self.weights = \n",
        "        \n",
        "        # Perform gradient descent\n",
        "        for i in range(self.max_iterations):\n",
        "            # Define the linear hypothesis(z) first\n",
        "            # HINT: what is our hypothesis function in linear regression, remember?\n",
        "            z = \n",
        "            \n",
        "            # Output probability value by appplying sigmoid on z\n",
        "            y_pred = \n",
        "            \n",
        "            # Calculate the gradient values\n",
        "            # This is just vectorized efficient way of implementing gradient. Don't worry, we will discuss it later.\n",
        "            gradient = np.mean((y-y_pred)*X.T, axis=1)\n",
        "            \n",
        "            # Update the weights using gradient descent\n",
        "            self.weights = \n",
        "            \n",
        "            # Calculating cross entropy or log-loss (negatie log likelihood)\n",
        "            cross_entropy = \n",
        "\n",
        "            self.cross_entropy_error.append(cross_entropy)\n",
        "    \n",
        "        ### END CODE HERE\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        '''Predict probabilities for given X.\n",
        "        Remember sigmoid returns value between 0 and 1.\n",
        "        Args:\n",
        "            X : Numpy array (num_samples, num_features)\n",
        "        Returns:\n",
        "            probabilities: Numpy array (num_samples,)\n",
        "        '''\n",
        "        if self.weights is None:\n",
        "            raise Exception(\"Fit the model before prediction\")\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        z = \n",
        "        probabilities = \n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return probabilities\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        '''Predict/Classify X in classes\n",
        "        Args:\n",
        "            X         : Numpy array (num_samples, num_features)\n",
        "            threshold : scalar value above which prediction is 1 else 0\n",
        "        Returns:\n",
        "            binary_predictions : Numpy array (num_samples,)\n",
        "        '''\n",
        "        # Thresholding probability to predict binary values\n",
        "        binary_predictions = np.array(list(map(lambda x: 1 if x>threshold else 0, self.predict_proba(X))))\n",
        "        \n",
        "        return binary_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crl6mGEoI5pJ"
      },
      "outputs": [],
      "source": [
        "# Now initialize logitic regression implemented by you\n",
        "model = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmMnrzPBI5pK"
      },
      "outputs": [],
      "source": [
        "# And now fit on training data\n",
        "model.fit(?, ?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK6tFOQTI5pL"
      },
      "source": [
        "##### Phew!! That's a lot of code. But you did it, congrats !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tvMc0OqwmSp"
      },
      "outputs": [],
      "source": [
        "# Training cross entropy cost (or log-loss)\n",
        "train_cross_entropy = model.cross_entropy(y_train, model.predict_proba(X_train))\n",
        "print(\"Cross entropy cost on training data:\", train_cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGViZYRDLcIZ"
      },
      "outputs": [],
      "source": [
        "# Testing cross entropy cost (or log-loss)\n",
        "test_cross_entropy = model.cross_entropy(y_test, model.predict_proba(X_test))\n",
        "print(\"Cross entropy cost on testing data:\", test_cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vnjkAvzI5pN"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curve\n",
        "plt.plot([i+1 for i in range(len(model.cross_entropy_error))], model.cross_entropy_error)\n",
        "plt.title(\"Cross entropy error curve\")\n",
        "plt.xlabel(\"Iteration num\")\n",
        "plt.ylabel(\"Cross entropy (-ve log-likelihood)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH-ocni8I5pO"
      },
      "source": [
        "##### Let's calculate accuracy as well. Accuracy is defined simply as the rate of correct classifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-RxIPbVI5pP"
      },
      "outputs": [],
      "source": [
        "#Make predictions on test data\n",
        "y_pred = model.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsS3PX65I5pP"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true,y_pred):\n",
        "    '''Compute accuracy.\n",
        "    Accuracy = (Correct prediction / number of samples)\n",
        "    Args:\n",
        "        y_true : Truth binary values (num_examples, )\n",
        "        y_pred : Predicted binary values (num_examples, )\n",
        "    Returns:\n",
        "        accuracy: scalar value\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    \n",
        "    accuracy = \n",
        "    ### END CODE HERE\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05SA_Ur6I5pQ"
      },
      "outputs": [],
      "source": [
        "# Print accuracy on train data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ0zLpChI5pQ"
      },
      "outputs": [],
      "source": [
        "# Print accuracy on test data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPU37tWAI5pR"
      },
      "source": [
        "## Part 1.2: Use Logistic Regression from sklearn on the same dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHscCs3bI5pR"
      },
      "source": [
        "#### Tasks\n",
        "- Define X and y again for sklearn Linear Regression model\n",
        "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
        "- Run the model on testing set\n",
        "- Print 'accuracy' obtained on the testing dataset (sklearn.metrics.accuracy_score function)\n",
        "\n",
        "#### Further fun (will not be evaluated)\n",
        "- Compare accuracies of your model and sklearn's logistic regression model\n",
        "\n",
        "#### Helpful links\n",
        "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcM8FakEI5pT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP8AhS_vI5pT"
      },
      "outputs": [],
      "source": [
        "# Define X and y\n",
        "X = \n",
        "y = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc7htjejI5pU"
      },
      "outputs": [],
      "source": [
        "# Initialize the model from sklearn\n",
        "model = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owIrjceyI5pU"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_YS9dn8I5pV"
      },
      "outputs": [],
      "source": [
        "# Predict on testing set X_test\n",
        "y_pred = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pBqDQ0qI5pW"
      },
      "outputs": [],
      "source": [
        "# Print Accuracy on testing set\n",
        "test_accuracy_sklearn = \n",
        "\n",
        "print(f\"\\nAccuracy on testing set: {test_accuracy_sklearn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28hkMN_MI5pX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "task_1_logistic_divorce.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
